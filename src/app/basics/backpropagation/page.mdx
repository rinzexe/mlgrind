# Backpropagation

Backpropagation is an (or more accurately, the) algorithm to apply gradient descent to hidden layers.
As hidden layers aren't correlated with inputs/outputs, you can't figure out their delta on their own.
<br />
<br />
Simply put, backpropagation calculates the gradient of the final layer's weights, and then changes the previous layer's weights to comply with the calulated gradient.
This process is repeated through every layer, changing every weight with the gradient.
<br />
<br />
Backpropagation is also one of those highly complicated things that aren't supposed to be memorized, so as long as you understand its basic concepts you'll be fine.
